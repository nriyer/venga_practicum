set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
}
wordcloud.func <- function(ACQstop, doc)
{
dtm <- TermDocumentMatrix(ACQstop)
m <- as.data.frame(as.matrix(dtm))
m <- m[,top10]
m <- as.matrix(m)
v <- sort(m[,doc],decreasing=TRUE)
d <- data.frame(word = row.names(m),freq=v)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
}
for (i in 1:10){
wordcloud.func(ACQstop,i)
}
wordcloud.func <- function(ACQstop, doc)
{
dtm <- TermDocumentMatrix(ACQstop)
m <- as.data.frame(as.matrix(dtm))
m <- m[,top10]
m <- as.matrix(m)
v <- sort(m[,doc],decreasing=TRUE)
d <- data.frame(word = row.names(m),freq=v)
set.seed(1234)
print(wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2")))
}
Q
wordcloud.func <- function(ACQstop, doc)
{
dtm <- TermDocumentMatrix(ACQstop)
m <- as.data.frame(as.matrix(dtm))
m <- m[,top10]
m <- as.matrix(m)
v <- sort(m[,doc],decreasing=TRUE)
d <- data.frame(word = row.names(m),freq=v)
set.seed(1234)
print(wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2")))
}
for (i in 1:10){
wordcloud.func(ACQstop,i)
}
top10_docs <- sort_top10[1:10,]
#tokenize the top 10 docs then look for the longest word
tokened <- tokenize(topdocs,what = c("word"))
tokened <- tokenize(top10_docs,what = c("word"))
top10_docs <- sort_top10[1:10,]
#tokenize the top 10 docs then look for the longest word
tokened <- tokenize(topdocs,what = c("word"))
a <- tokened[[2]]
top10_docs
inspect(topdocs)
topdocs
topdocs[[2]]
names(topdocs[1])
#split into sentences
get_sentence_df_func <- function(x){
sentence_df <- data.frame(sentence = character(0),
document = character(0))
for (i in 1:10){
temp <- data.frame(sentence=tokenize_sentences(x[[i]][[1]]),id=names(x[i]))
sentence_df <- rbind(sentence_df,temp)
}
return(sentence_df)
}
#data frame of sentences by document
text_sent <- get_sentence_df_func(topdocs)
View(text_sent)
text_sent <- get_sentence_df_func(topdocs)
#word count for each sentence
text_sent$sentence <- as.character(text_sent$sentence)
count <- c()
sapply(strsplit(text_sent$sentence[23], " "), length)
for (i in 1:nrow(text_sent)){
count[i] <- sapply(strsplit(text_sent$sentence[i], " "), length)
}
#length of each sentence in each document
count_sentences <- cbind(count,text_sent)
longest_10 <- count_sentences %>% group_by(id) %>%
arrange(desc(count)) %>% top_n(1,count) %>% distinct(id)
longest10
longest_10
sort(longest_10)
View(longest_10)
longest_10 <- longest_10 %>% arrange(desc(count))
longest_10
as.table(longest_10)
max_length <- c()
word <- c()
for (i in 1:10){
words <- tokened[[i]]
word[i] <- words[nchar(words) == max(nchar(words))]
max_length[i] <- max(nchar(words))
}
final.longest_word <- data.frame(max_length = max_length,word=word)
tokened <- tokenize(topdocs,what = c("word"))
names(tokened)
topdocs <- mycorpus[mycorpus$documents$id %in% top10]
topdocs[[1]]
tokened <- tokenize(topdocs,what = c("word"))
str(topdocs)
mycorpus <- corpus(acq)
summary_acq <- as.data.frame(summary(mycorpus))
sort_top10 <- summary_acq %>% arrange(desc(Tokens))
top_10_docs <- subset(sort_top10, select=c(id, heading))[1:10,]
top10 <- top_10_docs[,1]
topdocs <- mycorpus[mycorpus$documents$id %in% top10]
topdocs[[1]]
str(topdocs)
str(topdocs[[1]])
tokened <- tokenize(topdocs[[1]],what = c("word"))
library(tm)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(quanteda)
library(wordnet)
library(RColorBrewer)
library(NLP)
library(openNLP)
library(textreuse)
data("acq")
head(acq)
tokened <- tokenize(topdocs,what = c("word"))
names(tokened)
str(topdocs)
get_word_df_func <- function(x){
word_df <- data.frame(word = character(0),
document = character(0))
for (i in 1:10){
temp <- data.frame(word=tokenize_words(x[[i]][[1]]),id=names(x[i]))
word_df <- rbind(word_df,temp)
}
return(word_df)
}
get_word_df_func(topdocs)
word_df <- get_word_df_func(topdocs)
View(word_df)
max_length <- c()
word <- c()
for (i in 1:10){
words <- tokenize_words(x[[i]][[1]])
word[i] <- words[nchar(words) == max(nchar(words))]
max_length[i] <- max(nchar(words))
}
max_length <- c()
word <- c()
for (i in 1:10){
words <- tokenize_words(topdocs[[i]][[1]])
word[i] <- words[nchar(words) == max(nchar(words))]
max_length[i] <- max(nchar(words))
}
final.longest_word <- data.frame(max_length = max_length,word=word)
View(final.longest_word)
max_length <- c()
word <- c()
for (i in 1:10){
words <- tokenize_words(topdocs[[i]][[1]])
word[i] <- words[nchar(words) == max(nchar(words))]
max_length[i] <- max(nchar(words))
}
final.longest_word <- data.frame(max_length = max_length,word=word)
View(final.longest_word)
max_length <- c()
word <- c()
id <- c()
for (i in 1:10){
words <- tokenize_words(topdocs[[i]][[1]])
word[i] <- words[nchar(words) == max(nchar(words))]
max_length[i] <- max(nchar(words))
id[i] <- topdocs[i]
}
topdocs[1]
topdocs[[1]]
topdocs[1][1]
topdocs[[1]][1]
final.longest_word <- data.frame(max_length = max_length,word=word,id = id)
View(final.longest_word)
topdocs[[1]][1]
names(topdocs[1])
max_length <- c()
word <- c()
id <- c()
for (i in 1:10){
words <- tokenize_words(topdocs[[i]][[1]])
word[i] <- words[nchar(words) == max(nchar(words))]
max_length[i] <- max(nchar(words))
id[i] <- names(topdocs[i])
}
final.longest_word <- data.frame(max_length = max_length,word=word,id = id)
View(final.longest_word)
View(text_sent)
str(count_sentences$sentence)
count_sentences$sentence_nopunct[4] <- ( gsub("[[:punct:]]", "", count_sentences$sentence[4]) )
#loop to remove punctuation from each sentence
nopunct <- c()
for (i in 1:nrow(count_sentences)){
nopunct[i] <-  ( gsub("[[:punct:]]", "", count_sentences$sentence[i]) )
}
#bind final df together
final_nopunct_df <- cbind(nopunct,count_sentences)
View(final_nopunct_df)
View(final.longest_word)
all_words <- c()
id.all <- c()
for (i in 1:10){
all_words <- tokenize_words(topdocs[[i]][[1]])
id.all[i] <- names(topdocs[i])
}
all.words <- cbind(all_words,id.all)
all.words <- as.data.frame(all_words)
View(all.words)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all_words)
install.packages("openNLPdata")
library(openNLPdata)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all_words)
library(openNLP)
library(openNLPdata)
library(textreuse)
library(NLP)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all_words)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all_words[1,])
all_words[]
all_words[1,]
all.words[1,]
str(all.words)
all.words$all_words <- as.character(all.words)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all.words[1,])
Maxent_POS_Tag_Annotator(language = "en", probs = FALSE, model = NULL)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all.words[1,])
all.words[1,]
View(all.words)
all_words <- c()
id.all <- c()
for (i in 1:10){
all_words <- tokenize_words(topdocs[[i]][[1]])
id.all[i] <- names(topdocs[i])
}
all.words <- as.data.frame(all_words)
all.words$all_words <- as.character(all.words)
View(all.words)
for (i in 1:10){
all_words <- tokenize_words(topdocs[[i]][[1]])
id.all[i] <- names(topdocs[i])
}
all.words <- as.data.frame(all_words)
View(all.words)
all.words$all_words <- as.character(all.words$all_words)
Maxent_POS_Tag_Annotator(language = "en", probs = FALSE, model = NULL)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all.words[1,])
install.packages("openNLPmodels.investor")
library(openNLPmodels.investor)
Maxent_POS_Tag_Annotator(language = "en", probs = FALSE, model = NULL)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(all.words[1,])
pos_tag_annotator
setwd("~/Desktop/practicum/venga_practicum")
##functions
same_colnames <- function(df1,df2){
colnames <- colnames(df1)
colnames.same <- df2[,-which(colnames(df1) %in% colnames)]
print(names(colnames.same))
}
same_colnames(model.step.train,model.step)
#define RMSE function
RMSE <- function(predicted, true) mean((predicted-true)^2)^.5
setwd("~/Desktop/GW_spring2016/data analysis/project 2")
library(reshape)
library(ggplot2)
library(caret)
library(randomForest)
library(MASS)
library(data.table)
library(bit64)
library(corrplot)
library(plyr)
library(ROCR)
library(cluster)
library(ROSE)
library(car)
library(ROCR)
library(bestglm)
library(klaR)
library(glmnet)
library(tsne)
library(mgcv)
options(scipen = 999)
fact.table <- as.data.frame(fread("fact_table.csv"))
setwd("~/Desktop/practicum/venga_practicum")
fact.table <- as.data.frame(fread("fact_table.csv"))
fact.table <- as.data.frame(fread("fact_table.csv"))
View(fact.table)
str(fact.table)
fact.table$V1 <- NULL
fact.table$X <- NULL
row <-nrow(fact.table)
trainindex <- sample(row, 0.9*row, replace=FALSE)
train <- fact.table[trainindex,]
test  <- fact.table[-trainindex,]
sum(is.na(train))
set.seed(12345)
nzv <- nearZeroVar(train)
look.nzv <- train[,nzv]
View(look.nzv)
boxplot(x = look.nzv1[,30])
boxplot(x = look.nzv[,30])
train.var <- train[-nzv]
test.var <- test[-nzv]
str(train.var)
train.factor<- train[ , grepl( "flag" , names(train) ) ]
train.factor<- train.var[ , grepl( "flag" , names(train) ) ]
train.factor<- train.var[ , grepl( "flag" , names(train.var) ) ]
View(train.factor)
train.factor <- as.data.frame(sapply(train.factor, as.factor))
train.var <- train.var[,-which(names(train.var) %in% names(train.factor))]
train.ready <- cbind(train.var,train.factor)
train.ready$last_server_name <- as.factor(train.ready$last_server_name)
train.ready$first_server_name <- as.factor(train.ready$first_server_name)
str(test.var)
test.factor<- test.var[ , grepl( "flag" , names(test.var) ) ]
test.factor <- as.data.frame(sapply(test.factor, as.factor))
test.var <- test.var[,-which(names(test.var) %in% names(test.factor))]
test.ready <- cbind(test.var,test.factor)
test.ready$last_server_name <- as.factor(test.ready$last_server_name)
test.ready$first_server_name <- as.factor(test.ready$first_server_name)
str(train.ready)
train.corr <- train.ready[sapply(train.ready, is.numeric)]
descrCor <- cor(train.corr)
View(descrCor)
train.corr$loyalty_user_id <- NULL
descrCor <- cor(train.corr)
summary(descrCor[upper.tri(descrCor)])
corrplot(descrCor, order = "FPC", method = "color", type = "lower", tl.cex = 0.7, tl.col = rgb(0, 0, 0))
highlyCorrelated <- findCorrelation(descCor, cutoff = .9)
highlyCorrelated <- findCorrelation(descrCor, cutoff = .9)
look.corr<- train.corr[,highlyCorrelated]
View(look.corr)
View(train.corr)
highlyCorCol <- colnames(train.corr)[highlyCorrelated]
highlyCorCol
table(train.ready$has.repeated)
prop.table(table(train.ready$has.repeated))
str(train.ready$has.repeated)
train.ready$has.repeated <- as.factor(train.ready$has.repeated)
rf <- randomForest(has.repeated ~ ., data = train.ready,importance=TRUE,ntree=100)
rf$confusion
View(train.ready)
train.rf <- train.ready
train.rf$num_of_repeats <- NULL
rf <- randomForest(has.repeated ~ ., data = train.ready,importance=TRUE,ntree=100)
rf <- randomForest(has.repeated ~ ., data = train.rf,importance=TRUE,ntree=100)
rf$confusion
imp <- importance(rf,type=1)
imp <- data.frame(predictors=rownames(imp),imp)
View(imp)
imp.sort$predictors <- factor(imp.sort$predictors,levels=imp.sort$predictors)
imp.sort <- arrange(imp,desc(MeanDecreaseAccuracy))
imp.sort$predictors <- factor(imp.sort$predictors,levels=imp.sort$predictors)
View(imp.sort)
train.rf$loyalty_user_id <- NULL
rf <- randomForest(has.repeated ~ ., data = train.rf,importance=TRUE,ntree=100)
rf$confusion
imp <- importance(rf,type=1)
imp <- data.frame(predictors=rownames(imp),imp)
imp.sort <- arrange(imp,desc(MeanDecreaseAccuracy))
imp.sort$predictors <- factor(imp.sort$predictors,levels=imp.sort$predictors)
varImpPlot(rf.train, type=1)
varImpPlot(rf, type=1)
test.rf <- test.ready
test.rf$num_of_repeats <- NULL
test.rf$loyalty_user_id <- NULL
test.rf$has.repeated <- as.factor(test.ready$has.repeated)
test.rf <- test.ready
test.rf$num_of_repeats <- NULL
test.rf$loyalty_user_id <- NULL
predictions <- as.data.frame(predict(rf,test.rf,type="response"))
fact.table <- as.data.frame(fread("fact_table.csv"))
fact.table$V1 <- NULL
fact.table$X <- NULL
row <-nrow(fact.table)
trainindex <- sample(row, 0.9*row, replace=FALSE)
train <- fact.table[trainindex,]
test  <- fact.table[-trainindex,]
set.seed(12345)
nzv <- nearZeroVar(train)
look.nzv <- train[,nzv]
boxplot(x = look.nzv[,30])
dev.off()
boxplot(x = look.nzv[,30])
train.var <- train[-nzv]
test.var <- test[-nzv]
train.factor<- train.var[ , grepl( "flag" , names(train.var) ) ]
train.factor <- as.data.frame(sapply(train.factor, as.factor))
train.var <- train.var[,-which(names(train.var) %in% names(train.factor))]
train.ready <- cbind(train.var,train.factor)
train.ready$last_server_name <- as.factor(train.ready$last_server_name)
train.ready$first_server_name <- as.factor(train.ready$first_server_name)
#and for test.var
str(test.var)
test.factor<- test.var[ , grepl( "flag" , names(test.var) ) ]
test.factor <- as.data.frame(sapply(test.factor, as.factor))
test.var <- test.var[,-which(names(test.var) %in% names(test.factor))]
test.ready <- cbind(test.var,test.factor)
test.ready$last_server_name <- as.factor(test.ready$last_server_name)
test.ready$first_server_name <- as.factor(test.ready$first_server_name)
train.ready$has.repeated <- as.factor(train.ready$has.repeated)
test.ready$has.repeated <- as.factor(test.ready$has.repeated)
train.rf <- train.ready
train.rf$num_of_repeats <- NULL
train.rf$loyalty_user_id <- NULL
test.rf <- test.ready
test.rf$num_of_repeats <- NULL
test.rf$loyalty_user_id <- NULL
rf <- randomForest(has.repeated ~ ., data = train.rf,importance=TRUE,ntree=100)
rf$confusion
imp <- importance(rf,type=1)
imp <- data.frame(predictors=rownames(imp),imp)
imp.sort <- arrange(imp,desc(MeanDecreaseAccuracy))
imp.sort$predictors <- factor(imp.sort$predictors,levels=imp.sort$predictors)
varImpPlot(rf, type=1)
imp.20 <- imp.sort[1:20,]
#see the highly correlated variables really do no affect the prediction as much
#predict on test
predictions <- as.data.frame(predict(rf,test.rf,type="response"))
str(train.rf)
look <- as.data.framestr(train.rf)
look <- as.data.frame(str(train.rf))
str(train.rf)
str(test.rf)
same_colnames <- function(df1,df2){
colnames <- colnames(df1)
colnames.same <- df2[,-which(colnames(df1) %in% colnames)]
print(names(colnames.same))
}
same_colnames(train.rf,test.rf)
same_colnames <- function(df1,df2){
colnames <- colnames(df1)
colnames.same <- df2[,-!which(colnames(df1) %in% colnames)]
print(names(colnames.same))
}
same_colnames(train.rf,test.rf)
same_colnames <- function(df1,df2){
colnames <- colnames(df1)
colnames.same <- df2[,-!which(colnames(df1) %in% colnames)]
print(names(colnames.same))
}
same_colnames(train.rf,test.rf)
test.rf$last_server_name <- NULL
test.rf$first_server_name <- NULL
rf <- randomForest(has.repeated ~ ., data = train.rf,importance=TRUE,ntree=100)
rf$confusion
imp <- importance(rf,type=1)
imp <- data.frame(predictors=rownames(imp),imp)
imp.sort <- arrange(imp,desc(MeanDecreaseAccuracy))
imp.sort$predictors <- factor(imp.sort$predictors,levels=imp.sort$predictors)
varImpPlot(rf, type=1)
imp.20 <- imp.sort[1:20,]
predictions <- as.data.frame(predict(rf,test.rf,type="response"))
train.rf <- train.ready
train.rf$num_of_repeats <- NULL
train.rf$loyalty_user_id <- NULL
train.rf$last_server_name <- NULL
train.rf$first_server_name <- NULL
test.rf <- test.ready
test.rf$num_of_repeats <- NULL
test.rf$loyalty_user_id <- NULL
test.rf$last_server_name <- NULL
test.rf$first_server_name <- NULL
rf <- randomForest(has.repeated ~ ., data = train.rf,importance=TRUE,ntree=100)
rf$confusion
predictions <- as.data.frame(predict(rf,test.rf,type="response"))
table(predictions)
pred.forest <- predict(rf,type="prob",newdata=test.rf)
pred.forest.roc <- prediction(pred.forest[,2],test.rf$has.repeated)
pred.forest.performance <- performance(pred.forest.roc,"tpr","fpr")
plot(pred.forest.performance, col=3)
performance(pred.forest.roc,"auc")@y.values[[1]] #AUC
View(predictions)
View(pred.forest)
View(train.rf)
str(train.ready)
#separate out all numeric variables
train.corr <- train.ready[sapply(train.ready, is.numeric)]
#remove loyalty user id
train.corr$loyalty_user_id <- NULL
#calculate correlation matrix
descrCor <- cor(train.corr)
#look at max correlation
summary(descrCor[upper.tri(descrCor)])
highlyCorrelated <- findCorrelation(descrCor, cutoff = .9)
#24 variables that are over .9 correlated
look.corr<- train.corr[,highlyCorrelated]
#these are highly orrelated
highlyCorCol <- colnames(train.corr)[highlyCorrelated]
highlyCorCol
train.rf1 <- train.rf[,-which(colnames(train.rf) %in% highlyCorCol)]
test.rf1 <- test.rf[,-which(colnames(test.rf) %in% highlyCorCol)]
same_colnames(train.rf1,test.rf1)
rf1 <- randomForest(has.repeated ~ ., data = train.rf1,importance=TRUE,ntree=100)
rf1$confusion
predictions1 <- as.data.frame(predict(rf1,test.rf1,type="response"))
table(predictions1)
View(look.nzv)
varImpPlot(rf, type=1)
